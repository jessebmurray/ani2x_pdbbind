# %load_ext autoreload
# %autoreload 2


###################
# Forward Hook

# features = {}
# def get_features(name):
#     def hook(model, input, output):
#         features[name] = output.detach()
#     return hook
# h = model_ani2x['H'][4].register_forward_hook(get_features('H'))
###################


###################
# Energy shifter

# sae_file = os.path.join(path, 'resources/ani-2x_8x/sae_linfit.dat')  # noqa: E501
# energy_shifter = torchani.neurochem.load_sae(sae_file)
# nnp_ani2x = torchani.nn.Sequential(aev_computer, model_ani2x, energy_shifter)
###################


###################
# Histograms of parameters

# plt.hist([f for f in model_rand['P'].parameters()][0].ravel().detach().numpy(), bins=50);
# plt.hist([f for f in model_rand['S'].parameters()][0].ravel().detach().numpy(), bins=50);
# plt.imshow([f for f in model_pre['S'].parameters()][0].detach().numpy());
# plt.imshow([f for f in model_pre['F'].parameters()][0].detach().numpy());
###################



# mol = Chem.MolFromMol2File(mol_file)
# mol = Chem.AddHs(mol, addCoords=True)
# Ligand = LoadMolasDF(mol=mol)


###################
# To be readded

# learning_rate = 0.00001
# optimizer_pre = torch.optim.Adam(model_pre.parameters(), lr=learning_rate)
# optimizer_rand = torch.optim.Adam(model_rand.parameters(), lr=learning_rate)
# latest_checkpoint = 'latest.pt'

# if os.path.isfile(latest_checkpoint):
#     checkpoint = torch.load(latest_checkpoint)
#     model_pre.load_state_dict(checkpoint['model_pre'])
#     model_rand.load_state_dict(checkpoint['model_rand'])
###################

# atom_keys = pd.read_csv("../Data/PDB_Atom_Keys.csv", sep=",")



###################

# dfa = pd.concat(ligands)
# dfae = dfa.groupby('pdb').species.unique()
# dfae.map(lambda l: set(l).issubset(set(species_ani2x))).sum()
# dfae.shape
# 4510 / 5316
# species_ex = set(dfa.species.unique()) - set(species_ani2x)
# dfa.shape[0] - dfa.loc[dfa.species.isin(species_ex)].pdb.nunique()

# dfa.groupby('species').pdb.count()
# species_ani2x

# for i, dfl in enumerate(ligands):
#     dfl['pdb'] = pd.Series(np.repeat(pdbs[i], dfl.shape[0]))

# refined = pd.read_excel("../Data/pdb_refined_set.xlsx", header=1)

###################

# species, coordinates = training[0]['species'], training[0]['coordinates']
# aevs = aev_computer_ani2x.forward((species.unsqueeze(0), coordinates.unsqueeze(0)))

# model_pre2((species.unsqueeze(0), coordinates.unsqueeze(0)))
# species.unsqueeze(0)
# model_pre((species.unsqueeze(0), aevs.aevs))
# model_pre(aevs)

###################

# model_pre['H'][-1].weight.requires_grad = False
# model_pre['H'][-1].bias.requires_grad
# [f for f in model_pre.named_parameters()]
# model_pre.state_dict().keys()

# torch.load('./results_pre/best.pth', map_location=torch.device('cpu'))


###################


# def load_casf_split(distance_cutoff, batchsize):
#     df_gen = get_df_gen()
#     df_training, df_validation = split_by_casf(df_gen)
#     data_training, failed_entries_training = load_data(distance_cutoff, df_training)
#     data_validation, failed_entries_validation = load_data(distance_cutoff, df_validation)
#     save_list(failed_entries_training, 'failed_entries_training')
#     save_list(failed_entries_validation, 'failed_entries_validation')
#     trainloader, validloader = get_data_loaders(data_training, data_validation, batchsize)
#     return trainloader, validloader

###################

# def get_list(name):
#     with open(f'./losses/{name}.txt', 'r') as f:
#         lines = f.readlines()
#         lines = [float(line.rstrip('\n')) for line in lines]
#     return lines


###################


# n_atoms_each = []
# problem_pdb_ids = dict()
# i = 0
# for pdb_id in df_gen.index:
#     pdb_file = get_file(pdb=pdb_id, kind='protein', df_gen=df_gen)
#     ppdb_df = PandasPdb().read_pdb(pdb_file)
#     unique_atoms = set(ppdb_df.df['ATOM']['element_symbol'].unique())
#     n_atoms = len(unique_atoms)
#     n_atoms_each.append(n_atoms)
#     if n_atoms < 5:
#         problem_pdb_ids[pdb_id] = unique_atoms
#     i += 1
#     if i % 1_000 == 0:
#         print(i/df_gen.shape[0])

###################

# def get_protein_df(pdb, df_gen):
#     colspecs = [(0, 6), (6, 11), (12, 16), (16, 17), (17, 20), (21, 22), (22, 26),
#             (26, 27), (30, 38), (38, 46), (46, 54), (54, 60), (60, 66), (76, 78),
#             (78, 80)]
#     names = ['ATOM', 'serial', 'name', 'altloc', 'resname', 'chainid', 'resseq',
#          'icode', 'x', 'y', 'z', 'occupancy', 'tempfactor', 'element', 'charge']
#     pdb_file = get_file(pdb=pdb, kind='protein', df_gen=df_gen)
#     df = pd.read_fwf(pdb_file, names=names, colspecs=colspecs)
#     df = df.loc[df.ATOM == 'ATOM']
#     df = df.drop(columns='ATOM')
#     df = df.rename(columns = {'element': 'species'})
#     df = df.astype({'x': 'float32', 'y': 'float32', 'z': 'float32'})
#     if not set(df.species.unique()).issubset(SPECIES_ANI2X):
#         raise WrongElements
#     return df

###################

# sns.pairplot(df_gen.replace(to_replace={'R-factor':-1, 'R-free':-1, 'delta-R':-1}, value=np.nan).select_dtypes(include=float),
#             kind='reg', plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.01}});
# plt.savefig('./pdbbind_floats.png', dpi=320)


###################


# def get_df_gen():
#     general_file = '../Data/v2020-other-PL/index/INDEX_general_PL_data.2020'
#     refined_file = '../Data/v2020-other-PL/index/INDEX_refined_data.2020'
#     df_gen = get_pdb_entries(general_file)
#     df_ref = get_pdb_entries(refined_file)
#     refined_entry_ids = set(df_ref.index)
#     general_entry_ids = set(df_gen.index)
#     assert set(refined_entry_ids).issubset(set(general_entry_ids))
#     df_gen['Refined'] = df_gen.index.isin(refined_entry_ids)
#     df_gen['ID'] = pd.Series(np.arange(df_gen.shape[0]) + 1, index=df_gen.index)
#     return df_gen



###################

# def get_pdb_entries(file):
#     with open(file, 'r') as f:
#         lines = f.readlines()
#     entries = list()
#     for line in lines[6:]:
#         entry = line.split('  ')
#         # Can add parser for Ki<10mM stuff by going [:5]
#         entry = entry[:4]
#         a, b, c, d = entry
#         entry = {'Entry ID': a.upper(),
#                 'Resolution': float(b) if not b == ' NMR' else np.NAN,
#                 'Release Year': int(c),
#                 'pK': float(d)}
#         entries.append(entry)
#     return pd.DataFrame(entries).set_index('Entry ID')



###################


# def get_heteroatom_info(df):
#     df_ue = df.groupby('PDB_ID').element_symbol.unique().map(lambda element_list: ', '.join(element_list)).to_frame().reset_index()
#     df_ue = df_ue.groupby('element_symbol').PDB_ID.count().sort_values(ascending=False).to_frame().reset_index().rename(columns={'PDB_ID': 'Count'})
#     df_re = df.groupby('residue_name').element_symbol.value_counts().sort_values(ascending=False).to_frame().rename(columns={'element_symbol': 'Count'}).reset_index()
#     # df_re.to_csv('./residue_element.csv')
#     # df_ue.to_csv('./heteroatom_counts.csv')
#     return df_ue, df_re


###################

# def get_pdb_df(pdb, df_gen, atom_kind='protein'):
#     assert atom_kind in {'protein', 'hetatm'}
#     ppdb_df_label = {'protein': 'ATOM', 'hetatm': 'HETATM'}
#     pdb_file = get_file(pdb=pdb, kind='protein', df_gen=df_gen)
#     df = PandasPdb().read_pdb(pdb_file).df[ppdb_df_label[atom_kind]]
#     atom_column = 'element_symbol'
#     if set(df.element_symbol.unique()) == {''}:
#         df['atom_name']= df['atom_name'].map(
#             lambda an_entry: _get_element_atom_name(an_entry, atom_kind=atom_kind))
#         atom_column = 'atom_name'
#     df = df.rename(columns = {atom_column: 'species', 'x_coord': 'x', 'y_coord': 'y', 'z_coord': 'z'})
#     df = df.astype({'x': 'float32', 'y': 'float32', 'z': 'float32'})
#     return df


###################

# def load_data_6_angstrom_refined(remove_casf=True):
#     data = load_data_6_angstrom()
#     data = remove_many_atoms(data)
#     data = remove_low_resolution(data)
#     data = filter_casf(data, remove=remove_casf)
#     return data

# def save_data_6_angstrom(include_hetatms=False):
#     assert DISTANCE_CUTOFF == 6
#     df_gen = get_df_gen()
#     data, failed_entries = load_data(DISTANCE_CUTOFF, df_gen, include_hetatms=include_hetatms)
#     for entry in data:
#         entry['species'] = list(entry['species'].numpy())
#         entry['coordinates'] = [list(l) for l in entry['coordinates'].numpy()]
#     df_data = pd.DataFrame(data)
#     df_data = df_data.set_index('ID')
#     df_gen = df_gen.reset_index().set_index('ID')
#     df_data = df_data.join(df_gen)
#     hetatms_str = 'w_hetatms_' if include_hetatms else ''
#     df_data.to_csv(f'./data/pdb_bind_ani2x_6_angstrom_{hetatms_str}backup.csv')


# def load_df_data_6_angstrom():
#     n_files = 5
#     dataframes = []
#     for i in range(n_files):
#         file_number = i + 1
#         file_path = f'./data/pdb_bind_ani2x_6_angstrom_{file_number}.csv'
#         dataframes.append(pd.read_csv(file_path))
#     df_data = pd.concat(dataframes)
#     df_data = df_data.set_index('ID')
#     return df_data

# def load_data_6_angstrom():
#     # pdb_bind_path = './data/pdb_bind_ani2x_6_angstrom.csv'
#     # df_data = pd.read_csv(pdb_bind_path, index_col=0)
#     df_data = load_df_data_6_angstrom()
#     data = df_data.reset_index().to_dict(orient='records')
#     for entry in data:
#         entry['species'] = torch.from_numpy(np.array(json.loads(entry['species'])))
#         entry['coordinates'] = torch.from_numpy(np.array(json.loads(entry['coordinates']))).to(torch.float32)
#     return data

# def remove_many_atoms(data, percentage=0.02):
#     # largest was 1426 atoms
#     sizes_dict = {entry['ID']: len(entry['species']) for entry in data}
#     sizes_series = pd.Series(sizes_dict).sort_values()
#     sizes_series = sizes_series[:-int(sizes_series.shape[0] * percentage)]
#     data_refined = [entry for entry in data if entry['ID'] in set(sizes_series.index)]
#     return data_refined

# def filter_casf(data, remove=True):
#     casf = load_casf()
#     if remove:
#         return [entry for entry in data if entry['PDB_ID'] not in casf]
#     return [entry for entry in data if entry['PDB_ID'] in casf]

# def remove_low_resolution(data, resolution_threshold=3.5):
#     data_refined = [entry for entry in data if entry['Resolution'] < resolution_threshold]
#     return data_refined

# class WrongLigandElements(Exception):
#     pass
# class WrongProteinElements(Exception):
#     pass
# class EmptyLigand(Exception):
#     pass
# class SelenoProtein(Exception):
#     pass

# def _get_element_atom_name(an_entry, atom_kind='protein'):
#     assert atom_kind in {'protein', 'hetatm'}
#     if atom_kind == 'protein':
#         return re.match(r'\d*(H|C|N|O|S)', an_entry).group(1)
#     elif atom_kind == 'hetatm':
#         match = re.match(r'\d*(O|N|ZN)', an_entry)
#         if not match:
#             return None
#         return match.group(1)

# def get_ligand_df(pdb, df_gen):
#     mol_file = get_file(pdb=pdb, kind='ligand', df_gen=df_gen)
#     df = PandasMol2().read_mol2(mol_file).df
#     df['species'] = df.atom_type.str.split('.').str[0]
#     df = df.astype({'x': 'float32', 'y': 'float32', 'z': 'float32'})
#     if set(df.species.unique()) == set():
#         print(pdb)
#         raise EmptyLigand
#     if not set(df.species.unique()).issubset(SPECIES_ANI2X):
#         raise WrongLigandElements
#     return df


# def get_species_coordinates(pdb, distance_cutoff, df_gen, water=False):
#     protein = get_pdb_df(pdb, df_gen, 'protein')
#     if include_hetatms:
#         hetatms = get_pdb_df(pdb, df_gen, 'hetatm')
#     ligand = get_ligand_df(pdb, df_gen)
#     for i in ["x","y","z"]:
#         protein = protein[protein[i] < float(ligand[i].max())+distance_cutoff]
#         protein = protein[protein[i] > float(ligand[i].min())-distance_cutoff]
#         if include_hetatms:
#             hetatms = hetatms[hetatms[i] < float(ligand[i].max())+distance_cutoff]
#             hetatms = hetatms[hetatms[i] > float(ligand[i].min())-distance_cutoff]
#     # Reduce to just the neural network elements
#     if 'Se' in set(protein['species']):
#         print(pdb)
#         raise SelenoProtein
#     if include_hetatms:
#         hetatms = hetatms[hetatms.species.isin(SPECIES_ANI2X)]
#         df_list = [protein, ligand, hetatms]
#     else:
#         df_list = [protein, ligand]
#     structure = pd.concat(df_list, join='inner')
#     if not set(protein.species).issubset(SPECIES_ANI2X):
#         print(pdb)
#         raise WrongProteinElements
#     # structure = structure.loc[structure.species.isin(SPECIES_ANI2X)]
#     coordinates = torch.tensor(structure[['x','y','z']].values) #.unsqueeze(0)
#     consts_ani2x = get_consts_ani2x()
#     species = consts_ani2x.species_to_tensor(structure.species.values) #.unsqueeze(0)
#     return species, coordinates

# def _get_id(df_gen, pdb):
#     return df_gen.loc[pdb].ID

# def _get_binding_affinity(df_gen, pdb):
#     return  df_gen.loc[pdb].pK


# def get_entry(pdb, distance_cutoff, df_gen, water=False):
#     species, coordinates = get_species_coordinates(pdb, distance_cutoff,
#                                 df_gen, water=water)
#     affinity = _get_binding_affinity(df_gen, pdb)
#     id_ = _get_id(df_gen, pdb)
#     entry = {'species': species, 'coordinates': coordinates,
#             'affinity': affinity, 'ID': id_}
#     return entry

# def load_data(distance_cutoff, df_gen, water=False):
#     data = []
#     failed_entries = []
#     for pdb in df_gen.index:
#         try:
#             entry = get_entry(pdb, distance_cutoff,
#                               df_gen, water=water)
#             data.append(entry)
#         except WrongLigandElements:
#             pass
#         except WrongProteinElements:
#             pass
#         except SelenoProtein:
#             pass
#         except:
#             failed_entries.append(pdb)
#     return data, failed_entries

# def get_file(pdb, kind, df_gen):
#     folder_lookup = {'refined': 'refined-set', 'general': 'v2020-other-PL'}
#     kind_lookup = {'ligand': 'ligand.mol2', 'protein': 'pocket.pdb'}
#     kind_text = kind_lookup[kind]
#     subset = _get_subset(df_gen, pdb)
#     folder = folder_lookup[subset]
#     file = f'../Data/{folder}/{pdb.lower()}/{pdb.lower()}_{kind_text}'
#     return file

# def get_aevs_from_file(file_name):
#     consts = torchani.neurochem.Constants(file_name)
#     aev_computer = torchani.AEVComputer(**consts)
#     return aev_computer, consts

# def split_data():
#     n_files = 5
#     pdb_bind_path = './data/pdb_bind_ani2x_6_angstrom.csv'
#     df_data = pd.read_csv(pdb_bind_path, index_col=0)
#     df_data = df_data.reset_index()
#     n_interval = df_data.shape[0] // n_files
#     startings = np.arange(0, n_interval*n_files, n_interval)
#     df_data_1 = df_data[startings[0]: startings[1]]
#     df_data_2 = df_data[startings[1]: startings[2]]
#     df_data_3 = df_data[startings[2]: startings[3]]
#     df_data_4 = df_data[startings[3]:startings[4]]
#     df_data_5 = df_data[startings[4]:]
#     df_data_1.to_csv('./data/pdb_bind_ani2x_6_angstrom_1.csv', index=False)
#     df_data_2.to_csv('./data/pdb_bind_ani2x_6_angstrom_2.csv', index=False)
#     df_data_3.to_csv('./data/pdb_bind_ani2x_6_angstrom_3.csv', index=False)
#     df_data_4.to_csv('./data/pdb_bind_ani2x_6_angstrom_4.csv', index=False)
#     df_data_5.to_csv('./data/pdb_bind_ani2x_6_angstrom_5.csv', index=False)


###################



###################
